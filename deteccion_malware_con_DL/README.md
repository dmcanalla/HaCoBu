# Hackathon: *Deep Learning aplicado a Ciberseguridad Forense*

## Descripción del reto

En este hackathon intensivo de 24 horas, los participantes se enfrentarán a un desafío aplicado al ámbito de la **ciberseguridad forense**, con un enfoque en el uso de **Deep Learning sobre datos tabulares** extraídos de entornos reales comprometidos.

Los datos suministrados provienen de la **extracción de características a partir de capturas de memoria de máquinas infectadas**, procesadas mediante herramientas de análisis forense. Estas capturas corresponden a escenarios controlados donde se han ejecutado distintos tipos de malware, lo que permite construir un conjunto de datos etiquetado para tareas de clasificación supervisada.

## Objetivo

Desarrollar un modelo de **Deep Learning** capaz de clasificar muestras en una de las siguientes cuatro categorías:

- `0`: Benigno  
- `1`: Ransomware  
- `2`: Spyware  
- `3`: Troyano  

La métrica de evaluación será el **F1 Score**, calculado sobre las predicciones del conjunto de test.

## Infraestructura técnica

Para facilitar una experimentación ágil y reproducible, se proporciona un **código template modular** en GitHub. Este código incluye las herramientas necesarias para definir y ejecutar experimentos de forma configurable mediante archivos `.yaml`.

### Reglas

- **El uso del código base proporcionado es obligatorio.**  
  Su estructura modular permite sustituir componentes individuales (modelos, funciones de pérdida, normalizaciones, etc.) sin necesidad de modificar el pipeline completo. Esto facilita la comparación entre distintas aproximaciones y la revisión técnica por parte del jurado.
  
- **El código puede ser modificado libremente**, siempre que se respete la lógica modular original. Se recomienda mantener una logica clara y consistente.

- **Está permitido el uso de asistentes de programación** (como GitHub Copilot), lo que puede acelerar significativamente el desarrollo bajo presión temporal.

- **Está totalmente prohibido** el uso de datos externos

- El **modelo presentado debe circunscribirse a soluciones Deep Learning, no valiendo métodos Machine Learning, como árboles de decision**. El csv entregado tendrá que ser reproducible con el modelo DL propuesto como solución. Se revisarán las soluciones, reproduciendo la generación del csv de submission.

## Seguimiento de experimentación

El código está integrado con **Weights & Biases (wandb)**, plataforma que se utilizará para registrar todas las ejecuciones de entrenamiento y validación, junto con las métricas asociadas. Su uso es **obligatorio** y será una **herramienta clave para la evaluación de los equipos**.

### Configuración de wandb

1. Crear una cuenta en [wandb.ai](https://wandb.ai).
2. Definir el `entity` correspondiente en el archivo `.yaml`.
3. Crear una carpeta `wandb/` y añadir un archivo `login.json` con la clave de API personal:


{"key": "TU_API_KEY"}

> El repositorio incluye un experimento funcional con el dataset **CIFAR-10**, destinado a verificar el correcto funcionamiento del entorno.

## Estrategia recomendada

Dada la naturaleza competitiva y limitada en tiempo del hackathon, se recomienda la siguiente estrategia de desarrollo:

1. **Revisión rápida del estado del arte (SOTA)** en Deep Learning para datos tabulares.

2. **Definición de un modelo baseline**  
   El primer paso práctico debe ser la implementación de un modelo simple pero funcional, que pueda entrenarse y validarse rápidamente. Este baseline debe mantenerse como referencia (`baseline.yaml`).

3. **Diseño e implementación de una estrategia de validación robusta**  
   Se recomienda el uso de `k-fold cross-validation`, aunque su implementación queda a criterio del equipo según la disponibilidad de tiempo.

4. **Experimentación**  
   Probar nuevas combinaciones de:
   - Modelos
   - Transformaciones
   - Embeddings
   - Optimizadores
   - Schedulers

5. **Evaluación crítica en wandb**  
   Analizar los resultados obtenidos y rediseñar los experimentos de forma iterativa en función del desempeño.

6. **Exportación final**  
   Al término del hackathon, cada equipo deberá seleccionar su mejor modelo y generar un archivo `.csv` con su **nombre de equipo**, conteniendo las **etiquetas predichas** para el conjunto de test, separadas por comas.  
   Este archivo se enviará a los organizadores para la elaboración del **ranking final**.

---

## Evaluación y criterios de calificación

- La métrica principal será el **F1 Score** sobre el conjunto de test.

- Además del rendimiento cuantitativo, se valorará la **calidad técnica de la experimentación**:

  - Se espera la incorporación de **técnicas y modelos del estado del arte (SOTA)** en Deep Learning aplicado a datos tabulares.

  - **No se aceptarán modelos que no se basen en Deep Learning**, como árboles de decisión, random forests o boosting.  
    El uso de estos enfoques supondrá la **descalificación** del ejercicio.

  - Se evaluará la **trazabilidad del trabajo a través de wandb**, la **calidad del código** y la **claridad en la configuración de los experimentos**.

    

----------

## **Notas Finales**

- Se recomienda empezar con el modelo mas sencillo posible (DL) y establecer un baseline claro antes de probar ideas con el fin de poder establecer cuales son y no son útiles.

- Se recomienda basarse en implementaciones de torch de los modelos o técnicas del SOTA que seleccioneis

- Recursos:

Papers y guithub de yandex-research

https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html

